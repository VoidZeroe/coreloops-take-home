# Coreloops AI Engineer Take-Home

A production-ready data pipeline and ML system for customer transaction forecasting.

## Project Structure

```
coreloops-takehome/
├── src/
│   ├── __init__.py
│   ├── data/
│   │   ├── __init__.py
│   │   ├── loader.py          # Data ingestion from GCS
│   │   ├── cleaner.py          # Deduplication & missing value handling
│   │   └── aggregator.py       # Daily customer metrics
│   ├── features/
│   │   ├── __init__.py
│   │   └── engineering.py      # Feature generation for ML
│   ├── models/
│   │   ├── __init__.py
│   │   ├── trainer.py          # Model training logic
│   │   └── predictor.py        # Prediction interface
│   └── utils/
│       ├── __init__.py
│       └── fx.py               # FX rate conversion
├── scripts/
│   ├── run_pipeline.py         # Main ETL pipeline
│   └── predict.py              # CLI prediction tool
├── tests/
│   ├── __init__.py
│   ├── test_cleaner.py
│   └── test_aggregator.py
├── artifacts/                  # Generated by pipeline
│   ├── daily_customer_metrics.parquet
│   ├── daily_customer_metrics.csv
│   └── model/
│       ├── model.joblib
│       ├── metadata.json
│       └── feature_importance.csv
├── requirements.txt
├── Dockerfile
├── .gitignore
├── pipeline.log                # Generated during execution
└── README.md
```

## Setup

### Prerequisites
- Python 3.9+
- pip or conda

### Installation

```bash
# Clone the repository
git clone <repo-url>
cd coreloops-takehome

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## Usage

### 1. Run the Full Pipeline

This will:
- Download all daily transaction files from GCS
- Load and normalize FX rates
- Clean and deduplicate data
- Aggregate into daily customer metrics
- Engineer features and train the model

```bash
python -m scripts.run_pipeline
```

### 2. Make Predictions

```bash
python -m scripts.predict --customer C00042 --date 2024-10-06
```


## Design Decisions

### Data Cleaning

**Deduplication Strategy:**
- Criteria: Duplicate rows defined as having identical `invoice_no`, `stock_code`, `quantity`, and `invoice_date`
- Approach: Keep first occurrence, log duplicate count
- Rationale: True duplicates are likely data entry errors; variations in these fields indicate distinct transactions

**Missing Value Handling:**

1. **customer_id**: 
   - Drop rows with missing customer_id
   - Rationale: Customer-level prediction requires valid customer identifier

2. **unit_price**:
   - Impute using median price per (stock_code, currency) over trailing 7 days
   - Fallback to global median for that stock_code if insufficient data
   - Drop if still missing (typically <1% of data)
   - Rationale: Prices are relatively stable short-term; product-specific medians preserve value distribution

3. **description**:
   - Fill with "UNKNOWN" category
   - Rationale: Non-critical field; preserves transaction for revenue calculation

### FX Conversion

- All monetary values converted to GBP using daily rates from `fx_rates.csv`
- Transaction date matched to FX rate date
- Missing FX rates handled by forward-filling last known rate
- Conversion traceable via logged rate application

### Feature Engineering

**Temporal Features:**
- Rolling 7-day, 14-day, 30-day averages of net_gbp, gross_gbp, returns_gbp
- Days since last transaction
- Transaction frequency (count per rolling window)

**Customer Behavior:**
- Return rate (returns_gbp / gross_gbp)
- Average order value
- Item diversity (unique products per period)

**Calendar Features:**
- Day of week
- Month
- Is weekend

**Rationale:** Combines recency (recent behavior is predictive), frequency (engagement patterns), and monetary value (RFM framework)

### Model Choice

**Algorithm:** Gradient Boosting (LightGBM)

**Justification:**
- Handles non-linear relationships between features
- Robust to missing values and outliers
- Excellent performance on tabular data
- Fast training and inference
- Feature importance for interpretability

**Validation Strategy:**
- Time-based split: 80% train, 20% validation
- No data leakage (features computed only on past data)
- Walk-forward validation for temporal consistency

**Evaluation Metric:** 
- Primary: RMSE (penalizes large errors)
- Secondary: MAE (interpretable in GBP)
- R² score for explained variance
- ✓ Training samples: 603
- ✓ Test samples: 160
- ✓ Features: 41
- ✓ Test MAE: £22.95
- ✓ Test RMSE: £37.20
- ✓ Test R²: 0.8676
- ✓ Train MAE: £17.37
- ✓ Train RMSE: £26.19
- ✓ Train R²: 0.9355
 
### Production Considerations

1. **Modularity:** Clear separation of ETL, feature engineering, and ML
2. **Reproducibility:** Fixed random seeds, versioned data artifacts
3. **Observability:** Comprehensive logging at each pipeline stage
4. **Error Handling:** Graceful degradation with informative messages
5. **Scalability:** Parquet format for efficient storage and query
6. **Testing:** Unit tests for critical data transformations

## Key Assumptions

1. Daily files arrive complete (no late-arriving data)
2. FX rates are authoritative and complete
3. Negative quantities always represent returns (not corrections)
4. Customer behavior is somewhat stable short-term (enables forecasting)
5. Business days only (weekends may have zero transactions)

## Performance Expectations

- **Data Processing:** ~30s for 100 daily files
- **Model Training:** ~5s on typical dataset size
- **Prediction:** <100ms per customer-date pair


## Contact

For questions or issues, please reach out on git.